{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3340b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ARP_Spoofing_train.csv.csv  --> label: ARP_Spoofing_train.csv.csv\n",
      "Loading: Benign_train.csv.csv  --> label: Benign_train.csv.csv\n",
      "Loading: MQTT-DDoS-Connect_Flood_train.csv.csv  --> label: MQTT-DDoS-Connect_Flood_train.csv.csv\n",
      "Loading: MQTT-DDoS-Publish_Flood_train.csv.csv  --> label: MQTT-DDoS-Publish_Flood_train.csv.csv\n",
      "Loading: MQTT-DoS-Connect_Flood_train.csv.csv  --> label: MQTT-DoS-Connect_Flood_train.csv.csv\n",
      "Loading: MQTT-DoS-Publish_Flood_train.csv.csv  --> label: MQTT-DoS-Publish_Flood_train.csv.csv\n",
      "Loading: MQTT-Malformed_Data_train.csv.csv  --> label: MQTT-Malformed_Data_train.csv.csv\n",
      "Loading: Recon-OS_Scan_train.csv.csv  --> label: Recon-OS_Scan_train.csv.csv\n",
      "Loading: Recon-Ping_Sweep_train.csv.csv  --> label: Recon-Ping_Sweep_train.csv.csv\n",
      "Loading: Recon-Port_Scan_train.csv.csv  --> label: Recon-Port_Scan_train.csv.csv\n",
      "\n",
      "✅ Mixed+balanced training dataset saved to:\n",
      "D:\\Grad\\mixed_dataset\\mixed_train_balanced.csv\n",
      "\n",
      "Class counts used:\n",
      "ARP_Spoofing_train.csv.csv: 16047\n",
      "Benign_train.csv.csv: 20000\n",
      "MQTT-DDoS-Connect_Flood_train.csv.csv: 20000\n",
      "MQTT-DDoS-Publish_Flood_train.csv.csv: 20000\n",
      "MQTT-DoS-Connect_Flood_train.csv.csv: 12773\n",
      "MQTT-DoS-Publish_Flood_train.csv.csv: 20000\n",
      "MQTT-Malformed_Data_train.csv.csv: 5130\n",
      "Recon-OS_Scan_train.csv.csv: 16832\n",
      "Recon-Ping_Sweep_train.csv.csv: 740\n",
      "Recon-Port_Scan_train.csv.csv: 20000\n",
      "\n",
      "Total rows: 151522\n",
      "Columns: 46\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "DATA_DIR = r\"D:\\Grad\\Dataset\"              # where your *_train*.csv are\n",
    "OUTPUT_DIR = r\"D:\\Grad\\mixed_dataset\"\n",
    "SAMPLES_PER_CLASS = 20000                  # keep like before; can change later\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_samples = []\n",
    "class_counts = {}\n",
    "\n",
    "def infer_class_name(filename: str) -> str:\n",
    "    name = filename\n",
    "    for suffix in [\"_train.pcap.csv\", \"_train.csv\", \".pcap.csv\", \".csv\"]:\n",
    "        if name.lower().endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "            break\n",
    "    # IMPORTANT: keep same label style you used in training BEFORE\n",
    "    # e.g. \"Benign_train.csv\", \"Recon-Port_Scan_train.csv\"\n",
    "    # If your previous labels were exactly class_name + \"_train.csv\", keep it that way:\n",
    "    if not name.lower().endswith(\"_train\"):\n",
    "        # if file naming is different, do nothing\n",
    "        pass\n",
    "    return name + \".csv\"\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    f_lower = file.lower()\n",
    "\n",
    "    # Accept train CSVs (covers both *_train.csv and *_train.pcap.csv)\n",
    "    if not (f_lower.endswith(\".csv\") and \"_train\" in f_lower):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(DATA_DIR, file)\n",
    "    class_name = infer_class_name(file)\n",
    "\n",
    "    print(f\"Loading: {file}  --> label: {class_name}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Sample N rows per class (if file has fewer rows, take all)\n",
    "    if len(df) > SAMPLES_PER_CLASS:\n",
    "        df = df.sample(SAMPLES_PER_CLASS, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Add label column\n",
    "    df[\"Label\"] = class_name\n",
    "\n",
    "    all_samples.append(df)\n",
    "    class_counts[class_name] = len(df)\n",
    "\n",
    "if not all_samples:\n",
    "    raise SystemExit(\"No train CSV files found. Make sure filenames contain '_train' and end with .csv\")\n",
    "\n",
    "final_df = pd.concat(all_samples, ignore_index=True)\n",
    "\n",
    "# ✅ IMPORTANT: Shuffle rows to create a \"mixed stream\"\n",
    "final_df = final_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "out_path = os.path.join(OUTPUT_DIR, \"mixed_train_balanced.csv\")\n",
    "final_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\n✅ Mixed+balanced training dataset saved to:\")\n",
    "print(out_path)\n",
    "\n",
    "print(\"\\nClass counts used:\")\n",
    "for k, v in sorted(class_counts.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nTotal rows:\", len(final_df))\n",
    "print(\"Columns:\", final_df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097315d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: D:\\Grad\\derived_dataset\\train_balanced.csv\n",
      "Shape: (151522, 46)\n",
      "No leakage columns found among: ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp']\n",
      "Dropped duplicates: 974\n",
      "✅ Saved cleaned dataset to: D:\\Grad\\derived_dataset\\train_balanced_clean.csv\n",
      "Final shape: (150548, 46)\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "Benign_train.csv                     20000\n",
      "MQTT-DDoS-Connect_Flood_train.csv    20000\n",
      "MQTT-DoS-Publish_Flood_train.csv     20000\n",
      "MQTT-DDoS-Publish_Flood_train.csv    20000\n",
      "Recon-Port_Scan_train.csv            19695\n",
      "Recon-OS_Scan_train.csv              16163\n",
      "ARP_Spoofing_train.csv               16047\n",
      "MQTT-DoS-Connect_Flood_train.csv     12773\n",
      "MQTT-Malformed_Data_train.csv         5130\n",
      "Recon-Ping_Sweep_train.csv             740\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "INPUT_PATH = r\"D:\\Grad\\derived_dataset\\train_balanced.csv\"\n",
    "OUTPUT_PATH = r\"D:\\Grad\\derived_dataset\\train_balanced_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "print(\"Loaded:\", INPUT_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# ----- 1) Drop leakage / identifier columns if they exist -----\n",
    "leak_cols = [\"Flow ID\", \"Src IP\", \"Dst IP\", \"Timestamp\"]\n",
    "existing_leaks = [c for c in leak_cols if c in df.columns]\n",
    "if existing_leaks:\n",
    "    df = df.drop(columns=existing_leaks)\n",
    "    print(\"Dropped leakage columns:\", existing_leaks)\n",
    "else:\n",
    "    print(\"No leakage columns found among:\", leak_cols)\n",
    "\n",
    "# ----- 2) Ensure Label exists -----\n",
    "if \"Label\" not in df.columns:\n",
    "    raise ValueError(\"Label column not found. Make sure your dataset has a 'Label' column.\")\n",
    "\n",
    "# ----- 3) Replace infinities with NaN -----\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ----- 4) Handle missing values -----\n",
    "# Strategy: fill numeric NaNs with median (robust), non-numeric NaNs with mode\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "obj_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "# Don't impute labels\n",
    "if \"Label\" in num_cols:\n",
    "    num_cols.remove(\"Label\")\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        med = df[c].median()\n",
    "        df[c] = df[c].fillna(med)\n",
    "\n",
    "for c in obj_cols:\n",
    "    if c == \"Label\":\n",
    "        continue\n",
    "    if df[c].isna().any():\n",
    "        mode_val = df[c].mode(dropna=True)\n",
    "        df[c] = df[c].fillna(mode_val.iloc[0] if len(mode_val) else \"\")\n",
    "\n",
    "# ----- 5) Drop duplicate rows (optional but safe) -----\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped duplicates: {before - after}\")\n",
    "\n",
    "# ----- 6) Save cleaned dataset -----\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"✅ Saved cleaned dataset to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482aecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: D:\\Grad\\derived_dataset\\train_balanced_clean.csv\n",
      "Shape: (150548, 46)\n",
      "Features shape: (150548, 45)\n",
      "Labels shape: (150548,)\n",
      "\n",
      "Label mapping:\n",
      "                               Label  Encoded\n",
      "0             ARP_Spoofing_train.csv        0\n",
      "1                   Benign_train.csv        1\n",
      "2  MQTT-DDoS-Connect_Flood_train.csv        2\n",
      "3  MQTT-DDoS-Publish_Flood_train.csv        3\n",
      "4   MQTT-DoS-Connect_Flood_train.csv        4\n",
      "5   MQTT-DoS-Publish_Flood_train.csv        5\n",
      "6      MQTT-Malformed_Data_train.csv        6\n",
      "7            Recon-OS_Scan_train.csv        7\n",
      "8         Recon-Ping_Sweep_train.csv        8\n",
      "9          Recon-Port_Scan_train.csv        9\n",
      "\n",
      "✅ Saved:\n",
      "- X_scaled.npy\n",
      "- y_encoded.npy\n",
      "- label_mapping.csv\n",
      "- scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# ===== PATHS =====\n",
    "INPUT_PATH = r\"D:\\Grad\\derived_dataset\\train_balanced_clean.csv\"\n",
    "OUTPUT_DIR = r\"D:\\Grad\\derived_dataset\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----- Load cleaned data -----\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(\"Loaded:\", INPUT_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# ----- Separate features and labels -----\n",
    "X = df.drop(columns=[\"Label\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# ----- Encode labels -----\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save label mapping\n",
    "label_map = pd.DataFrame({\n",
    "    \"Label\": label_encoder.classes_,\n",
    "    \"Encoded\": range(len(label_encoder.classes_))\n",
    "})\n",
    "label_map_path = os.path.join(OUTPUT_DIR, \"label_mapping.csv\")\n",
    "label_map.to_csv(label_map_path, index=False)\n",
    "\n",
    "print(\"\\nLabel mapping:\")\n",
    "print(label_map)\n",
    "\n",
    "# ----- Scale features -----\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(OUTPUT_DIR, \"scaler.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# ----- Save processed arrays -----\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_scaled.npy\"), X_scaled)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y_encoded.npy\"), y_encoded)\n",
    "\n",
    "print(\"\\n✅ Saved:\")\n",
    "print(\"- X_scaled.npy\")\n",
    "print(\"- y_encoded.npy\")\n",
    "print(\"- label_mapping.csv\")\n",
    "print(\"- scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d863a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (150548, 45)\n",
      "Loaded y: (150548,)\n",
      "\n",
      "✅ Sequence dataset created\n",
      "X_seq shape: (30106, 20, 45)\n",
      "y_seq shape: (30106,)\n",
      "\n",
      "✅ Saved:\n",
      "- X_seq.npy\n",
      "- y_seq.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "INPUT_DIR = r\"D:\\Grad\\derived_dataset\"\n",
    "OUTPUT_DIR = r\"D:\\Grad\\derived_dataset\"\n",
    "\n",
    "X_path = os.path.join(INPUT_DIR, \"X_scaled.npy\")\n",
    "y_path = os.path.join(INPUT_DIR, \"y_encoded.npy\")\n",
    "\n",
    "X = np.load(X_path)\n",
    "y = np.load(y_path)\n",
    "\n",
    "print(\"Loaded X:\", X.shape)\n",
    "print(\"Loaded y:\", y.shape)\n",
    "\n",
    "# ===== SEQUENCE SETTINGS =====\n",
    "WINDOW_SIZE = 20\n",
    "STRIDE = 5\n",
    "\n",
    "def majority_vote(labels):\n",
    "    return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "for start in range(0, num_samples - WINDOW_SIZE + 1, STRIDE):\n",
    "    end = start + WINDOW_SIZE\n",
    "\n",
    "    window_X = X[start:end]\n",
    "    window_y = y[start:end]\n",
    "\n",
    "    X_seq.append(window_X)\n",
    "    y_seq.append(majority_vote(window_y))\n",
    "\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "y_seq = np.array(y_seq, dtype=np.int64)\n",
    "\n",
    "print(\"\\n✅ Sequence dataset created\")\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "\n",
    "# Save\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_seq.npy\"), X_seq)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y_seq.npy\"), y_seq)\n",
    "\n",
    "print(\"\\n✅ Saved:\")\n",
    "print(\"- X_seq.npy\")\n",
    "print(\"- y_seq.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d686cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30106, 20, 45)\n",
      "y: (30106,)\n",
      "num_classes: 10\n",
      "Train: (24084, 20, 45) (24084,)\n",
      "Val: (6022, 20, 45) (6022,)\n",
      "Class weights: {0: 0.9385814497272019, 1: 0.752625, 2: 0.752625, 3: 0.752625, 4: 1.1782778864970647, 5: 0.752625, 6: 2.933495736906212, 7: 0.931322505800464, 8: 20.410169491525423, 9: 0.7648142267386472}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m8,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,626</span> (342.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m87,626\u001b[0m (342.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,626</span> (342.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m87,626\u001b[0m (342.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8847 - loss: 0.4292 - val_accuracy: 0.9714 - val_loss: 0.0854\n",
      "Epoch 2/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9731 - loss: 0.1148 - val_accuracy: 0.9701 - val_loss: 0.0832\n",
      "Epoch 3/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9796 - loss: 0.0915 - val_accuracy: 0.9841 - val_loss: 0.0467\n",
      "Epoch 4/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9839 - loss: 0.0678 - val_accuracy: 0.9816 - val_loss: 0.0547\n",
      "Epoch 5/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9863 - loss: 0.0576 - val_accuracy: 0.9890 - val_loss: 0.0355\n",
      "Epoch 6/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9890 - loss: 0.0473 - val_accuracy: 0.9890 - val_loss: 0.0331\n",
      "Epoch 7/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9889 - loss: 0.0439 - val_accuracy: 0.9909 - val_loss: 0.0289\n",
      "Epoch 8/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0375 - val_accuracy: 0.9900 - val_loss: 0.0356\n",
      "Epoch 9/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9906 - loss: 0.0393 - val_accuracy: 0.9857 - val_loss: 0.0492\n",
      "Epoch 10/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9922 - loss: 0.0321 - val_accuracy: 0.9917 - val_loss: 0.0235\n",
      "Epoch 11/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9936 - loss: 0.0254 - val_accuracy: 0.9927 - val_loss: 0.0319\n",
      "Epoch 12/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9912 - loss: 0.0370 - val_accuracy: 0.9892 - val_loss: 0.0309\n",
      "Epoch 13/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9937 - loss: 0.0238 - val_accuracy: 0.9910 - val_loss: 0.0247\n",
      "Epoch 14/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9929 - loss: 0.0271 - val_accuracy: 0.9912 - val_loss: 0.0234\n",
      "Epoch 15/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9944 - loss: 0.0195 - val_accuracy: 0.9914 - val_loss: 0.0280\n",
      "Epoch 16/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9935 - loss: 0.0247 - val_accuracy: 0.9912 - val_loss: 0.0299\n",
      "Epoch 17/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9960 - loss: 0.0153 - val_accuracy: 0.9922 - val_loss: 0.0268\n",
      "Epoch 18/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9930 - loss: 0.0278 - val_accuracy: 0.9917 - val_loss: 0.0228\n",
      "Epoch 19/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0221 - val_accuracy: 0.9907 - val_loss: 0.0267\n",
      "Epoch 20/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9954 - loss: 0.0152 - val_accuracy: 0.9922 - val_loss: 0.0284\n",
      "Epoch 21/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9961 - loss: 0.0126 - val_accuracy: 0.9885 - val_loss: 0.0402\n",
      "Epoch 22/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9965 - loss: 0.0137 - val_accuracy: 0.9902 - val_loss: 0.0338\n",
      "Epoch 23/30\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0110 - val_accuracy: 0.9929 - val_loss: 0.0233\n",
      "✅ Saved model to: D:\\Grad\\model_output\\cnn_lstm_final.keras\n",
      "✅ Saved history to: D:\\Grad\\model_output\\training_history.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "DATA_DIR = r\"D:\\Grad\\derived_dataset\"\n",
    "MODEL_DIR = r\"D:\\Grad\\model_output\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load sequences ----\n",
    "X = np.load(os.path.join(DATA_DIR, \"X_seq.npy\"))\n",
    "y = np.load(os.path.join(DATA_DIR, \"y_seq.npy\"))\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "num_classes = int(np.max(y)) + 1\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# ---- Train/Val split (keep test separate if you have it later) ----\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# ---- Class weights (helps imbalance) ----\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "print(\"Class weights:\", class_weight)\n",
    "\n",
    "# ---- Build CNN-LSTM model ----\n",
    "timesteps = X.shape[1]   # 20\n",
    "features = X.shape[2]    # 45\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(timesteps, features)),\n",
    "\n",
    "    layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ---- Callbacks ----\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(MODEL_DIR, \"cnn_lstm_best.keras\"),\n",
    "        monitor=\"val_loss\", save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# ---- Train ----\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Save final model ----\n",
    "final_model_path = os.path.join(MODEL_DIR, \"cnn_lstm_final.keras\")\n",
    "model.save(final_model_path)\n",
    "print(\"✅ Saved model to:\", final_model_path)\n",
    "\n",
    "# ---- Save training history ----\n",
    "hist_path = os.path.join(MODEL_DIR, \"training_history.csv\")\n",
    "pd.DataFrame(history.history).to_csv(hist_path, index=False)\n",
    "print(\"✅ Saved history to:\", hist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9648166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification report (validation):\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "           ARP_Spoofing_train.csv     0.9717    0.9611    0.9663       642\n",
      "                 Benign_train.csv     1.0000    1.0000    1.0000       800\n",
      "MQTT-DDoS-Connect_Flood_train.csv     0.9963    1.0000    0.9981       800\n",
      "MQTT-DDoS-Publish_Flood_train.csv     1.0000    1.0000    1.0000       800\n",
      " MQTT-DoS-Connect_Flood_train.csv     1.0000    0.9941    0.9971       511\n",
      " MQTT-DoS-Publish_Flood_train.csv     1.0000    1.0000    1.0000       800\n",
      "    MQTT-Malformed_Data_train.csv     0.9752    0.9610    0.9681       205\n",
      "          Recon-OS_Scan_train.csv     0.9953    0.9861    0.9907       646\n",
      "       Recon-Ping_Sweep_train.csv     0.5714    0.9333    0.7089        30\n",
      "        Recon-Port_Scan_train.csv     1.0000    0.9962    0.9981       788\n",
      "\n",
      "                         accuracy                         0.9917      6022\n",
      "                        macro avg     0.9510    0.9832    0.9627      6022\n",
      "                     weighted avg     0.9930    0.9917    0.9921      6022\n",
      "\n",
      "\n",
      "✅ Confusion matrix (validation):\n",
      "[[617   0   0   0   0   0   4   1  20   0]\n",
      " [  0 800   0   0   0   0   0   0   0   0]\n",
      " [  0   0 800   0   0   0   0   0   0   0]\n",
      " [  0   0   0 800   0   0   0   0   0   0]\n",
      " [  0   0   3   0 508   0   0   0   0   0]\n",
      " [  0   0   0   0   0 800   0   0   0   0]\n",
      " [  8   0   0   0   0   0 197   0   0   0]\n",
      " [  8   0   0   0   0   0   1 637   0   0]\n",
      " [  2   0   0   0   0   0   0   0  28   0]\n",
      " [  0   0   0   0   0   0   0   2   1 785]]\n",
      "\n",
      "✅ Saved confusion matrix to model_output/confusion_matrix_val.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_DIR = r\"D:\\Grad\\derived_dataset\"\n",
    "MODEL_PATH = r\"D:\\Grad\\model_output\\cnn_lstm_final.keras\"\n",
    "LABEL_MAP_PATH = os.path.join(DATA_DIR, \"label_mapping.csv\")\n",
    "\n",
    "# Load data\n",
    "X = np.load(os.path.join(DATA_DIR, \"X_seq.npy\"))\n",
    "y = np.load(os.path.join(DATA_DIR, \"y_seq.npy\"))\n",
    "\n",
    "# Recreate the same split (must match Step 4)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Load label names\n",
    "label_map = pd.read_csv(LABEL_MAP_PATH)\n",
    "# label_map: Label, Encoded\n",
    "label_names = label_map.sort_values(\"Encoded\")[\"Label\"].tolist()\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Predict\n",
    "y_prob = model.predict(X_val, batch_size=256, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "print(\"✅ Classification report (validation):\")\n",
    "print(classification_report(y_val, y_pred, target_names=label_names, digits=4))\n",
    "\n",
    "print(\"\\n✅ Confusion matrix (validation):\")\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Save results\n",
    "out_dir = r\"D:\\Grad\\model_output\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(cm, index=label_names, columns=label_names).to_csv(\n",
    "    os.path.join(out_dir, \"confusion_matrix_val.csv\")\n",
    ")\n",
    "print(\"\\n✅ Saved confusion matrix to model_output/confusion_matrix_val.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
